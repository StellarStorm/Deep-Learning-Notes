<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Skylar's Programming Notes</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/</link><description>Recent content on Skylar's Programming Notes</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 17 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://stellarstorm.github.io/Deep-Learning-Notes/index.xml" rel="self" type="application/rss+xml"/><item><title>Misadventures in Modifying the Output of Input()</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/input_layer/</link><pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/input_layer/</guid><description>I (re-)noticed something odd today when modifying a custom U-Net model in TensorFlow 2. The goal was to downsample the input image and segmentation maps on the fly once they&amp;rsquo;re fed into the model during training, in order to (a) effectively increase the receptive fields of the kernels, but mostly to (b) fit more data per batch into a GPU that likes to run out of memory.
A simple downsampling operation is just to call tf.</description></item><item><title>Region-Overlap Metrics/Losses</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/region_loss/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/region_loss/</guid><description>Overlap metrics of two regions are excellent ways to quantify how well a model is able to predict segmentation mappings.
All code examples below are simply psuedo-code that has been simplified for clarity. For actual loss functions, RadIO has ready-to-go examples under the Apache 2.0 license.
Left: Intersection of two sets. Right: Union of two sets [1]
Jaccard The Jaccard index, also called IoU (Intersection over Union), is perhaps one of the easiest metrics to visualize.</description></item><item><title>About</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/about/</link><pubDate>Wed, 31 Mar 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/about/</guid><description>What is this? At the moment, this is essentially my deep learning notebook. It&amp;rsquo;s formatted something like a blog to make organizations easier, but I doubt this will become a &amp;ldquo;blog&amp;rdquo; in any real sense - posts will change and be updated as I gain experience and find errors. The hope is to have a unified location where I keep everything I&amp;rsquo;ve learned (with consistent formatting!)
This is very much an opinionated collection of information and should be treated as such!</description></item><item><title>Gradient Descent Optimizers</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</guid><description>Optimizers are crucial to training a Deep Learning model, but (for me anyways) they are neither chosen as carefully or even understood nearly so much as other components such as loss. This is understandable - you can&amp;rsquo;t visualize them the same way you can a loss function, for instance - but it&amp;rsquo;s a mistake. The optimizer is what drives the learning process itself! The loss measures how similar (or not) the output of your model is to the ground truth, but then it&amp;rsquo;s the job of the optimizer to adjust the model parameters so that the loss can be decreased over the next epochs, over and over again until the loss minima (and, hopefully, the optimal model) can be found.</description></item><item><title/><link>https://stellarstorm.github.io/Deep-Learning-Notes/archives/</link><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/archives/</guid><description/></item></channel></rss>