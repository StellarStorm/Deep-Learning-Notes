<!doctype html><html dir=ltr lang=en data-theme><head><title>Skylar Gay | Misadventures in Modifying the Output of Input()</title><meta charset=utf-8><meta name=generator content="Hugo 0.83.1"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="from . import skykit-learn"><link rel=stylesheet href=/Deep-Learning-Notes/css/main.min.f79a5de55310cc058fc81804d033ddb955937d778bd533d9ea05b1c798501013.css integrity="sha256-95pd5VMQzAWPyBgE0DPduVWTfXeL1TPZ6gWxx5hQEBM=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/Deep-Learning-Notes/css/markupHighlight.min.f798cbda9aaa38f89eb38be6414bd082cfd71a6780375cbf67b6d2fb2b96491e.css integrity="sha256-95jL2pqqOPies4vmQUvQgs/XGmeAN1y/Z7bS+yuWSR4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel="shortcut icon" href=/Deep-Learning-Notes/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/Deep-Learning-Notes/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/Deep-Learning-Notes/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/Deep-Learning-Notes/favicons/favicon-16x16.png><link rel=canonical href=/Deep-Learning-Notes/post/input_layer/><script type=text/javascript src=/Deep-Learning-Notes/js/anatole-header.min.0c05c0a90d28c968a1cad4fb31abd0b8e1264e788ccefed022ae1d3b6f627514.js integrity="sha256-DAXAqQ0oyWihytT7MavQuOEmTniMzv7QIq4dO29idRQ=" crossorigin=anonymous></script><script type=text/javascript src=/Deep-Learning-Notes/js/anatole-theme-switcher.min.7fd87181cdd7e8413aa64b6867bb32f3a8dc242e684fc7d5bbb9f600dbc2b6eb.js integrity="sha256-f9hxgc3X6EE6pktoZ7sy86jcJC5oT8fVu7n2ANvCtus=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Misadventures in Modifying the Output of Input() "><meta name=twitter:description content="I (re-)noticed something odd today when modifying a custom U-Net model in TensorFlow 2. The goal was to downsample the input image and segmentation maps on the fly once they&rsquo;re fed into the model during training, in order to (a) effectively increase the receptive fields of the kernels, but mostly to (b) fit more data per batch into a GPU that likes to run out of memory.
A simple downsampling operation is just to call tf."></head><body><div class="sidebar animated fadeInDown"><div class=logo-title><div class=title><img src=/Deep-Learning-Notes/images/profile.jpg alt="profile picture"><h3 title><a href=/>Skylar's Programming Notes</a></h3><div class=description><p>from . import skykit-learn</p></div></div></div><ul class=social-links><li><a href=https://gitlab.com/StellarStorm rel=me aria-label=GitLab><i class="fab fa-gitlab fa-2x" aria-hidden=true></i></a></li><li><a href=https://github.com/StellarStorm rel=me aria-label=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href=https://twitter.com/sgi386 rel=me aria-label=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li><a href=https://www.strava.com/athletes/69920138 rel=me aria-label=Strava><i class="fab fa-strava fa-2x" aria-hidden=true></i></a></li></ul><div class=footer><div class=by_farbox>&copy; Skylar Gay 2021</div></div></div><div class=main><div class="page-top animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span><span aria-hidden=true></span><span aria-hidden=true></span></a><ul class=nav id=navMenu><li><a href=/Deep-Learning-Notes/ title>Home</a></li><li><a href=/Deep-Learning-Notes/post/ title>Posts</a></li><li><a href=/Deep-Learning-Notes/about/ title>About Me</a></li><li class=theme-switch-item><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></div><div class=autopagerize_page_element><div class=content><div class="post animated fadeInDown"><div class=post-content><div class=post-title><h3>Misadventures in Modifying the Output of Input()</h3><div class=info><em class="fas fa-calendar-day"></em><span class=date>Thu, Jun 17, 2021</span>
<em class="fas fa-stopwatch"></em><span class=reading-time>3-minute read</span></div></div><p>I (re-)noticed something odd today when modifying a custom U-Net model in
TensorFlow 2. The goal was to downsample the input image and segmentation maps
on the fly once they&rsquo;re fed into the model during training, in order to (a)
effectively increase the receptive fields of the kernels, but mostly to (b) fit
more data per batch into a GPU that likes to run out of memory.</p><p>A simple downsampling operation is just to call
<code>tf.keras.layers.AveragePooling3D()</code> on the input tensors - that is,
do something like this:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>SimpleModel</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>pass</span>

    <span class=k>def</span> <span class=nf>model</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>inpt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
        <span class=n>inpt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>AveragePooling3D</span><span class=p>()(</span><span class=n>inpt</span><span class=p>)</span>

        <span class=c1># Reduce dimensions by factor of 2</span>
        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv3D</span><span class=p>(</span><span class=n>filters</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
                                   <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
                                   <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>inpt</span><span class=p>)</span>

        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>UpSampling3D</span><span class=p>()(</span><span class=n>c</span><span class=p>)</span>
        <span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>inpt</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=p>[</span><span class=n>c</span><span class=p>])</span>
        <span class=k>return</span> <span class=n>model</span>
</code></pre></div><p>Or so I thought. Imagine my surprise when I got this lovely error message:</p><blockquote><p><code>ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 64, 128, 128, 2), dtype=tf.float32, name='input_1'), name='input_1', description="created by layer 'input_1'") at layer "average_pooling3d". The following previous layers were accessed without issue: []</code></p></blockquote><p>What&rsquo;s going on? After all, if I just remove the average pooling layer and run
the convolution on <code>inpt</code>, it works just fine. Why does it error out when I
stick a different layer in the middle?</p><p>Spoiler alert: I don&rsquo;t know! (If you do, please
<a href=https://github.com/StellarStorm/Deep-Learning-Notes/issues>let me know</a>).
But there&rsquo;s still a way to get this to work. If we
first assign <code>inpt</code> to a new variable, and then operate on that instead, it
works just fine</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>SimpleModel</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>pass</span>

    <span class=k>def</span> <span class=nf>model</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>inpt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
        <span class=c1># Call `inpt` directly</span>
        <span class=n>fixed</span> <span class=o>=</span> <span class=n>inpt</span>
        <span class=n>fixed</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>AveragePooling3D</span><span class=p>()(</span><span class=n>fixed</span><span class=p>)</span>

        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv3D</span><span class=p>(</span><span class=n>filters</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
                                   <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
                                   <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>fixed</span><span class=p>)</span>

        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>UpSampling3D</span><span class=p>()(</span><span class=n>c</span><span class=p>)</span>
        <span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>inpt</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=p>[</span><span class=n>c</span><span class=p>])</span>
        <span class=k>return</span> <span class=n>model</span>
</code></pre></div><p>A few interesting things to note:</p><ol><li><p>Both <code>inpt</code> and <code>fixed</code> seem to point to the same object. If we print the
<code>id()</code> of the variables like this,</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>inpt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=k>print</span><span class=p>(</span><span class=nb>id</span><span class=p>(</span><span class=n>inpt</span><span class=p>))</span>
<span class=n>fixed</span> <span class=o>=</span> <span class=n>inpt</span>
<span class=k>print</span><span class=p>(</span><span class=nb>id</span><span class=p>(</span><span class=n>inpt</span><span class=p>),</span> <span class=nb>id</span><span class=p>(</span><span class=n>fixed</span><span class=p>))</span>
<span class=k>print</span><span class=p>(</span><span class=nb>id</span><span class=p>(</span><span class=n>inpt</span><span class=p>)</span> <span class=o>==</span> <span class=nb>id</span><span class=p>(</span><span class=n>fixed</span><span class=p>))</span>
<span class=n>fixed</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>AveragePooling3D</span><span class=p>()(</span><span class=n>fixed</span><span class=p>)</span>
</code></pre></div><p>we get something like this</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>140222044803792
140222044803792 140222044803792
True
</code></pre></div></li><li><p>Reassigning <code>inpt</code> to itself, that is, replacing <code>fixed = inpt</code>
with <code>inpt = inpt</code>, does not work.</p></li></ol><p><em>Side note: why not just downsample the image as part of the data generator or
tf.data pipeline?</em> That would probably make more sense in most cases, but I&rsquo;d
just wanted to do it this way. Plus, a modified version of this approach is good
for chaining multi-scale networks together, like a coarse-to-fine segmentation
network, where you probably don&rsquo;t want to modify the data itself.</p><h2 id=appendix>Appendix</h2><p>Fully-working demo</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>tensorflow</span> <span class=kn>as</span> <span class=nn>tf</span>

<span class=n>feat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>random_sample</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
<span class=n>lab</span><span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>feat</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>

<span class=k>class</span> <span class=nc>SimpleModel</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=k>pass</span>

    <span class=k>def</span> <span class=nf>model</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>inpt</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
        <span class=n>fixed</span> <span class=o>=</span> <span class=n>inpt</span>
        <span class=n>fixed</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>AveragePooling3D</span><span class=p>()(</span><span class=n>fixed</span><span class=p>)</span>

        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Conv3D</span><span class=p>(</span><span class=n>filters</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
                                   <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
                                   <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;same&#39;</span><span class=p>)(</span><span class=n>fixed</span><span class=p>)</span>

        <span class=n>c</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>UpSampling3D</span><span class=p>()(</span><span class=n>c</span><span class=p>)</span>
        <span class=n>model</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>models</span><span class=o>.</span><span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>inpt</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=p>[</span><span class=n>c</span><span class=p>])</span>
        <span class=k>return</span> <span class=n>model</span>

<span class=n>mod</span> <span class=o>=</span> <span class=n>SimpleModel</span><span class=p>()</span><span class=o>.</span><span class=n>model</span><span class=p>()</span>
<span class=n>mod</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>)</span>
<span class=n>mod</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>feat</span><span class=p>,</span> <span class=n>lab</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>4</span><span class=p>)</span>
</code></pre></div></div><div class=post-footer><div class=info><span class=separator><a class=category href=/Deep-Learning-Notes/categories/deep-learning/>deep learning</a></span>
<span class=separator><a class=tag href=/Deep-Learning-Notes/tags/code/>code</a><a class=tag href=/Deep-Learning-Notes/tags/segmentation/>segmentation</a><a class=tag href=/Deep-Learning-Notes/tags/tensorflow/>tensorflow</a></span></div></div></div></div></div></div><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>