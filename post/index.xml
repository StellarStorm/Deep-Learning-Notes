<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Skylar's Programming Blog</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/</link><description>Recent content in Posts on Skylar's Programming Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 16 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://stellarstorm.github.io/Deep-Learning-Notes/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Region-Overlap Metrics/Losses</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/region_loss/</link><pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/region_loss/</guid><description>Overlap metrics of two regions are excellent ways to quantify how well a model is able to predict segmentation mappings.
All code examples below are simply psuedo-code that has been simplified for clarity. For actual loss functions, RadIO has ready-to-go examples under the Apache 2.0 license.
Left: Intersection of two sets. Right: Union of two sets [1]
Jaccard The Jaccard index, also called IoU (Intersection over Union), is perhaps one of the easiest metrics to visualize.</description></item><item><title>Gradient Descent Optimizers</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</guid><description>Optimizers are crucial to training a Deep Learning model, but (for me anyways) they are neither chosen as carefully or even understood nearly so much as other components such as loss. This is understandable - you can&amp;rsquo;t visualize them the same way you can a loss function, for instance - but it&amp;rsquo;s a mistake. The optimizer is what drives the learning process itself! The loss measures how similar (or not) the output of your model is to the ground truth, but then it&amp;rsquo;s the job of the optimizer to adjust the model parameters so that the loss can be decreased over the next epochs, over and over again until the loss minima (and, hopefully, the optimal model) can be found.</description></item></channel></rss>