<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tensorflow on Skylar's Programming Notes</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/tags/tensorflow/</link><description>Recent content in tensorflow on Skylar's Programming Notes</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 17 Jun 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://stellarstorm.github.io/Deep-Learning-Notes/tags/tensorflow/index.xml" rel="self" type="application/rss+xml"/><item><title>Misadventures in Modifying the Output of Input()</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/input_layer/</link><pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/input_layer/</guid><description>I (re-)noticed something odd today when modifying a custom U-Net model in TensorFlow 2. The goal was to downsample the input image and segmentation maps on the fly once they&amp;rsquo;re fed into the model during training, in order to (a) effectively increase the receptive fields of the kernels, but mostly to (b) fit more data per batch into a GPU that likes to run out of memory.
A simple downsampling operation is just to call tf.</description></item><item><title>Gradient Descent Optimizers</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</guid><description>Optimizers are crucial to training a Deep Learning model, but (for me anyways) they are neither chosen as carefully or even understood nearly so much as other components such as loss. This is understandable - you can&amp;rsquo;t visualize them the same way you can a loss function, for instance - but it&amp;rsquo;s a mistake. The optimizer is what drives the learning process itself! The loss measures how similar (or not) the output of your model is to the ground truth, but then it&amp;rsquo;s the job of the optimizer to adjust the model parameters so that the loss can be decreased over the next epochs, over and over again until the loss minima (and, hopefully, the optimal model) can be found.</description></item></channel></rss>