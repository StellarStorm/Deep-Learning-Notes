<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pytorch on Skylar's Programming Notes</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/tags/pytorch/</link><description>Recent content in pytorch on Skylar's Programming Notes</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 30 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://stellarstorm.github.io/Deep-Learning-Notes/tags/pytorch/index.xml" rel="self" type="application/rss+xml"/><item><title>Gradient Descent Optimizers</title><link>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</link><pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate><guid>https://stellarstorm.github.io/Deep-Learning-Notes/post/grad_desc_optimizers/</guid><description>Optimizers are crucial to training a Deep Learning model, but (for me anyways) they are neither chosen as carefully or even understood nearly so much as other components such as loss. This is understandable - you can&amp;rsquo;t visualize them the same way you can a loss function, for instance - but it&amp;rsquo;s a mistake. The optimizer is what drives the learning process itself! The loss measures how similar (or not) the output of your model is to the ground truth, but then it&amp;rsquo;s the job of the optimizer to adjust the model parameters so that the loss can be decreased over the next epochs, over and over again until the loss minima (and, hopefully, the optimal model) can be found.</description></item></channel></rss>